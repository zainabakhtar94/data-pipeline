{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/05/28 21:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/28 21:41:53 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 21:42:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/28 21:42:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/28 21:42:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from textblob import TextBlob\n",
    "\n",
    "def preprocessing(lines):\n",
    "    words = lines.select(explode(split(lines.value, \"t_end\")).alias(\"word\"))\n",
    "    words = words.na.replace('', None)\n",
    "    words = words.na.drop()\n",
    "    words = words.withColumn('word', F.regexp_replace('word', r'http\\S+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '@\\w+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '#', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', 'RT', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', ':', ''))\n",
    "    return words\n",
    "\n",
    "# text classification\n",
    "def polarity_detection(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "def subjectivity_detection(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "def text_classification(words):\n",
    "    # polarity detection\n",
    "    polarity_detection_udf = udf(polarity_detection, StringType())\n",
    "    words = words.withColumn(\"polarity\", polarity_detection_udf(\"word\"))\n",
    "    # subjectivity detection\n",
    "    subjectivity_detection_udf = udf(subjectivity_detection, StringType())\n",
    "    words = words.withColumn(\"subjectivity\", subjectivity_detection_udf(\"word\"))\n",
    "    return words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create Spark session\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "\n",
    "    # read the tweet data from socket\n",
    "    lines = spark.readStream.format(\"socket\").option(\"host\", \"0.0.0.0\").option(\"port\", 5656).load()\n",
    "    # Preprocess the data\n",
    "    words = preprocessing(lines)\n",
    "    # text classification to define polarity and subjectivity\n",
    "    words = text_classification(words)\n",
    "\n",
    "    words = words.repartition(10)\n",
    "    query = words.writeStream.queryName(\"all_tweets\")\\\n",
    "        .outputMode(\"append\").format(\"parquet\")\\\n",
    "        .option(\"path\", \"./parc\")\\\n",
    "        .option(\"checkpointLocation\", \"./check\")\\\n",
    "        .trigger(processingTime='60 seconds').start()\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- retweet_count: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- friends_count: string (nullable = true)\n",
      " |-- followers_count: string (nullable = true)\n",
      " |-- statuses_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# initialise sparkContext\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('TwitterAggregations') \\\n",
    "    .config('spark.executor.memory', '2gb') \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# to read parquet file\n",
    "df = sqlContext.read.parquet('parc/')\n",
    "\n",
    "df.createOrReplaceTempView(\"ParquetTable\")\n",
    "df.printSchema()\n",
    "# df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.filter((df['full_text']=='')|df['full_text'].isNull()|isnan(df['full_text'])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------------+-----------+-----------+--------+-------------+---------------+--------------+\n",
      "|text|full_text|retweet_count|screen_name|description|location|friends_count|followers_count|statuses_count|\n",
      "+----+---------+-------------+-----------+-----------+--------+-------------+---------------+--------------+\n",
      "|   0|      314|            0|          0|        149|     193|            0|              0|             0|\n",
      "+----+---------+-------------+-----------+-----------+--------+-------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_columns=['text','full_text','retweet_count','screen_name','description','location','friends_count','followers_count','statuses_count']\n",
    "# Subsetting the required columns from the DataFrame\n",
    "df_strings_only=df.select(*select_columns)\n",
    "#  calculate all the missing values in the DataFrame, you can use the following command:\n",
    "df_strings_only.select([count(when((col(c)=='') | col(c).isNull() | isnan(c), c)).alias(c) for c in df_strings_only.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:==============================================>       (171 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|screen_name    |count|\n",
      "+---------------+-----+\n",
      "|Numrahkhalid   |24   |\n",
      "|AbdulQayyum998 |13   |\n",
      "|Arbab_Sajjad786|10   |\n",
      "|khanadel       |9    |\n",
      "|azadpakistan4  |7    |\n",
      "|Shafeeq83017532|6    |\n",
      "|YousifB48749828|6    |\n",
      "|aksidr         |4    |\n",
      "|Mahnoor1005    |4    |\n",
      "|Shamshair_ch   |4    |\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  frequencies of categorical variables\n",
    "#  Categorical variables are any string variables that exist in a dataset.\n",
    "df.groupBy(df['screen_name']).count().sort(desc(\"count\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting and creating a temporary DataFrame to eliminate any missing values\n",
    "# Dropping null descriptions\n",
    "df_temp_drop_na_description=df.filter((df['description']!='')&(df['description'].isNotNull()) &(~isnan(df['description'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                                        |count|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|RT @ImranKhanPTI: My name is Imran Khan, I am from Pakistan and I #SupportGaza                                                              |57   |\n",
      "|RT @LahoriElite: Thank you Imran Khan, Keyboard Warriors and Social Media? https://t.co/vxMxIawUzn                                          |24   |\n",
      "|RT @PTIPoliticsss: Imran Khan will be addressing a Jalsa in Shangla on 2nd June 2022.                                                       |20   |\n",
      "|RT @JehangirMirzaa: It took an Imran Khan after 70 odd years of 'independence', to lay bare the myth that top military generals are incorru…|15   |\n",
      "|RT @agentjay2009: You know it is game over for Imported Government when their announced Jalsas using all State resources flop while Imran K…|12   |\n",
      "|RT @PTIPoliticsss: Imran Khan will be addressing a Jalsa in Buner on 3rd June 2022.                                                         |11   |\n",
      "|RT @PTIofficial: Thread by @Zyyan_Ali on the compilation of video clips on Chairman Imran Khan's stance on Palestine and Israel. He mention…|10   |\n",
      "|RT @PTIPoliticsss: Huge crowd welcomed Imran Khan at today's Charsadda Workers Convention. https://t.co/rNEuPs6Vgs                          |9    |\n",
      "|RT @PTIofficial: If stealing their mandate wasn't enough, the Imported Government deprived the citizens of their right to protest pitting P…|8    |\n",
      "|RT @KaliwalYam: This is how Imran Khan saved Pakistan otherwise imported government's plan was to set Pakistan on fire.\n",
      "\n",
      "#CrimeMinisterReje…|8    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Subsetting the DataFrame to titles that are repeated more than one time\n",
    "df_temp_text_repeat = df.groupby(df['text']).count().filter(\"`count` >1\").sort(col(\"count\").desc()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_at', 'timestamp'),\n",
       " ('text', 'string'),\n",
       " ('full_text', 'string'),\n",
       " ('hashtags', 'array<string>'),\n",
       " ('retweet_count', 'string'),\n",
       " ('screen_name', 'string'),\n",
       " ('description', 'string'),\n",
       " ('location', 'string'),\n",
       " ('friends_count', 'float'),\n",
       " ('followers_count', 'string'),\n",
       " ('statuses_count', 'string')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Casting\n",
    "df_casted = df.withColumn('friends_count',df['friends_count'].cast(\"float\"))\n",
    "#After Casting\n",
    "df_casted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, text: string, full_text: string, retweet_count: string, screen_name: string, description: string, location: string, friends_count: string, followers_count: string, statuses_count: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median of friends_count is [176.0]\n"
     ]
    }
   ],
   "source": [
    "median=df_casted.approxQuantile('friends_count',[0.5],0.1)\n",
    "#Printing the Value\n",
    "print ('The median of friends_count is '+str(median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the distinct occurances of titles\n",
    "df.select('screen_name').distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------------------------------------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------------------------+--------+-------------+---------------+--------------+\n",
      "|created_at         |text                                                                                                                              |full_text|hashtags                                                                                                                                         |retweet_count|screen_name |description                            |location|friends_count|followers_count|statuses_count|\n",
      "+-------------------+----------------------------------------------------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------------------------+--------+-------------+---------------+--------------+\n",
      "|2022-05-29 18:30:15|RT @salma4ever16: I really dare Sharif Family to tweet something like this!! #امپورٹڈ__حکومت___نامنظور #SupportGaza  #imrankhanPTI|null     |[{\"text\":\"امپورٹڈ__حکومت___نامنظور\",\"indices\":[77,102]}, {\"text\":\"SupportGaza\",\"indices\":[103,115]}, {\"text\":\"imrankhanPTI\",\"indices\":[117,130]}]|0            |malik_007007|Social activist**\n",
      "Imran Khan supporter.|null    |160          |33             |3895          |\n",
      "+-------------------+----------------------------------------------------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------------------------+--------+-------------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "df.filter(df['text'].rlike('\\w*imran')).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pop=df_casted.agg({'friends_count': 'mean'}).collect()[0]['avg(friends_count)']\n",
    "count_obs= df_casted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.withColumn('mean_friends_count',lit(mean_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/18 20:47:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+---------------------+\n",
      "|decile_rank|min_statuses_count|max_statuses_count|count(statuses_count)|\n",
      "+-----------+------------------+------------------+---------------------+\n",
      "|          1|              8290|               998|                   20|\n",
      "|          2|              6384|              8192|                   20|\n",
      "|          3|            421778|              6326|                   20|\n",
      "|          4|             31084|               402|                   19|\n",
      "|          5|             24893|             31081|                   19|\n",
      "|          6|            232650|             24892|                   19|\n",
      "|          7|            211481|              2322|                   19|\n",
      "|          8|              1560|             20335|                   19|\n",
      "|          9|             12133|              1559|                   19|\n",
      "|         10|              1000|             11996|                   19|\n",
      "+-----------+------------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "# Step 1: Filtering the missing values\n",
    "df_with_newcols=df.filter( (df['full_text'].isNotNull()) & (~isnan(df['full_text'])) )\n",
    "# Step 2: Applying the window functions for calculating deciles\n",
    "df_with_newcols = df_with_newcols.select('location','friends_count','followers_count','statuses_count',ntile(10).over(Window.partitionBy().orderBy(df_with_newcols['statuses_count'].desc())).alias(\"decile_rank\"))\n",
    "\n",
    "# Step 3:Dispalying the values\n",
    "df_with_newcols.groupby(\"decile_rank\").agg(min('statuses_count').alias('min_statuses_count'),max('statuses_count').alias('max_statuses_count'),count('statuses_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (339783851.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1794/339783851.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    df_second_best.filter((df_second_best['created_at']==DatetimeConverter().convert('18:28:18.000Z') & (df_second_best['rank']==2)).show()\u001b[0m\n\u001b[0m                                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "df_second_best=df.withColumn('created_at_sec',minute('created_at'))\n",
    "# Step 4: Define partition function\n",
    "year_window = Window.partitionBy(df_second_best['created_at_sec']).orderBy(df['location'].desc())\n",
    "\n",
    "df_second_best=df_second_best.select('text', 'created_at',rank().over(year_window).alias(\"rank\"))\n",
    "# Step 6: Find the second best rating for the year 1970\n",
    "df_second_best.filter((df_second_best['created_at']==DatetimeConverter().convert('18:28:18.000Z') & (df_second_best['rank']==2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
